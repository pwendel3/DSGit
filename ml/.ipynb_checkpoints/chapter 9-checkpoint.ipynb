{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELLS AND WHISTLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X,y=make_moons()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_bias=np.c_[np.ones((X.shape[0],1)),X,\n",
    "             np.square(X[:,0]),np.square(X[:,1]),\n",
    "            X[:,0]**3,X[:,1]**3]\n",
    "\n",
    "m,n=X_bias.shape\n",
    "\n",
    "y=y.reshape(-1,1)\n",
    "\n",
    "np.random.seed(42)\n",
    "arr=np.arange(m)\n",
    "np.random.shuffle(arr)\n",
    "\n",
    "test_ratio=0.2\n",
    "test_size=int(m*test_ratio)\n",
    "\n",
    "X_train=X_bias[:-test_size]\n",
    "X_test=X_bias[-test_size:]\n",
    "\n",
    "y_train=y[:-test_size]\n",
    "y_test=y[-test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feed_batch(epoch,batch_index,batch_size):\n",
    "    \n",
    "    np.random.seed(epoch*batch_size)\n",
    "    arr=np.arange(m)\n",
    "    \n",
    "    np.random.shuffle(arr)\n",
    "    indices=[batch_index*batch_size,min((batch_index+1)*batch_size,m)]\n",
    "    \n",
    "    X_batch=X_train[indices[0]:indices[1]]\n",
    "    y_batch=y_train[indices[0]:indices[1]]\n",
    "    \n",
    "    return X_batch,y_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=tf.placeholder(tf.float32,shape=(None,n),name='X')\n",
    "y=tf.placeholder(tf.float32,shape=(None,1),name='y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_regression(X,y,initializer=None, seed=42, learning_rate=0.01):\n",
    "    n_inputs_including_bias=int(X.get_shape()[1])\n",
    "    with tf.name_scope('logistic_regression'):\n",
    "        with tf.name_scope('model'):\n",
    "            if initializer is None:\n",
    "                initializer=tf.random_uniform([n_inputs_including_bias,1],-1,1,seed=seed)\n",
    "            theta=tf.Variable(initializer,name='theta')\n",
    "            logits=tf.matmul(X,theta,name='logits')\n",
    "            y_proba=tf.sigmoid(logits)\n",
    "        with tf.name_scope('train'):\n",
    "            loss=tf.losses.log_loss(y,y_proba,scope='loss')\n",
    "            optimizer=tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            training_op=optimizer.minimize(loss)\n",
    "            loss_summary=tf.summary.scalar('log_loss',loss)\n",
    "        with tf.name_scope('init'):\n",
    "            init=tf.global_variables_initializer()\n",
    "        with tf.name_scope('save'):\n",
    "            saver=tf.train.Saver()\n",
    "    return y_proba, loss, training_op, loss_summary, init, saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def log_dir(prefix=''):\n",
    "    now=datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "    root_logdir='tf_logs'\n",
    "    if prefix:\n",
    "        prefix+= '-'\n",
    "    name=prefix+'run-'+now\n",
    "    return \"{}/{}/\".format(root_logdir,name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logdir=log_dir('logreg')\n",
    "\n",
    "y_proba, loss, training_op, loss_summary, init, saver=logistic_regression(X,y)\n",
    "\n",
    "file_writer=tf.summary.FileWriter(logdir,tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tLoss: 0.997331\n",
      "Epoch: 500 \tLoss: 0.321336\n",
      "Epoch: 1000 \tLoss: 0.266861\n",
      "Epoch: 1500 \tLoss: 0.231768\n",
      "Epoch: 2000 \tLoss: 0.204833\n",
      "Epoch: 2500 \tLoss: 0.183205\n",
      "Epoch: 3000 \tLoss: 0.165467\n",
      "Epoch: 3500 \tLoss: 0.150696\n",
      "Epoch: 4000 \tLoss: 0.138239\n",
      "Epoch: 4500 \tLoss: 0.127612\n",
      "Epoch: 5000 \tLoss: 0.118454\n",
      "Epoch: 5500 \tLoss: 0.110487\n",
      "Epoch: 6000 \tLoss: 0.1035\n",
      "Epoch: 6500 \tLoss: 0.0973223\n",
      "Epoch: 7000 \tLoss: 0.0918243\n",
      "Epoch: 7500 \tLoss: 0.0869018\n",
      "Epoch: 8000 \tLoss: 0.0824699\n",
      "Epoch: 8500 \tLoss: 0.0784599\n",
      "Epoch: 9000 \tLoss: 0.0748142\n",
      "Epoch: 9500 \tLoss: 0.0714844\n",
      "Epoch: 10000 \tLoss: 0.0684325\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "n_epochs=10001\n",
    "batch_size=16\n",
    "n_batches=int(np.ceil(m/batch_size))\n",
    "\n",
    "checkpoint_path='/tmp/my_logreg_model.ckpt'\n",
    "checkpoint_epoch_path=checkpoint_path+'.epoch'\n",
    "final_model_path='./my_logreg_model'\n",
    "\n",
    "with tf.Session() as sess:\n",
    "#     if os.path.isfile(checkpoint_epoch_path):\n",
    "#         with open(checkpoint_epoch_path,'rb') as f:\n",
    "#             start_epoch=int(f.read())\n",
    "#         print('Training was interrupted. Continuing at epoch',start_epoch)\n",
    "#         saver.restore(sess,checkpoint_path)\n",
    "#     else:\n",
    "    start_epoch=0\n",
    "    sess.run(init)\n",
    "        \n",
    "    for epoch in range(start_epoch,n_epochs):\n",
    "        for batch_index in range(n_batches):\n",
    "            X_batch,y_batch=feed_batch(epoch,batch_index,batch_size)\n",
    "            sess.run(training_op, feed_dict={X:X_batch,y:y_batch})\n",
    "        loss_val, summary_str=sess.run([loss,loss_summary],feed_dict={X:X_test,y:y_test})\n",
    "        file_writer.add_summary(summary_str,epoch)\n",
    "        \n",
    "        if epoch%500==0:\n",
    "            print(\"Epoch:\", epoch,\"\\tLoss:\",loss_val)\n",
    "            saver.save(sess,checkpoint_path)\n",
    "            with open(checkpoint_epoch_path,'wb') as f:\n",
    "                f.write(b'%d'%(epoch+1))\n",
    "    y_proba_val=y_proba.eval(feed_dict={X:X_test,y:y_test})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
